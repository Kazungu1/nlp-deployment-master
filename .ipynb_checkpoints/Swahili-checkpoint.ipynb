{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from nltk.corpus import stopwords\n",
    "#from wordcloud import WordCloud ,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kazungu\\Anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23268, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SW4670</td>\n",
       "      <td>Bodi ya Utalii Tanzania (TTB) imesema, itafan...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SW30826</td>\n",
       "      <td>PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SW29725</td>\n",
       "      <td>Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SW20901</td>\n",
       "      <td>TIMU ya taifa ya Tanzania, Serengeti Boys jan...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SW12560</td>\n",
       "      <td>Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content category\n",
       "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
       "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
       "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
       "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
       "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['uchumi', 'kitaifa', 'michezo', 'kimataifa', 'burudani', 'afya'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    \n",
    "    'uchumi':0, 'kitaifa':1, 'michezo':2, 'kimataifa':3, 'burudani':4, 'afya':5\n",
    "}\n",
    "data[\"category_encoded\"] = data.category.map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SW4670</td>\n",
       "      <td>Bodi ya Utalii Tanzania (TTB) imesema, itafan...</td>\n",
       "      <td>uchumi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SW30826</td>\n",
       "      <td>PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...</td>\n",
       "      <td>kitaifa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SW29725</td>\n",
       "      <td>Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...</td>\n",
       "      <td>uchumi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SW20901</td>\n",
       "      <td>TIMU ya taifa ya Tanzania, Serengeti Boys jan...</td>\n",
       "      <td>michezo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SW12560</td>\n",
       "      <td>Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...</td>\n",
       "      <td>kitaifa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content category  \\\n",
       "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi   \n",
       "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa   \n",
       "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi   \n",
       "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo   \n",
       "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa   \n",
       "\n",
       "   category_encoded  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 0  \n",
       "3                 2  \n",
       "4                 1  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(words):\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import pickle\n",
    "# # from wordcloud import WordCloud\n",
    "# from nltk.corpus import stopwords\n",
    "# from wordcloud import WordCloud ,STOPWORDS\n",
    "# subset=data[data.category==\"uchumi\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"kitaifa\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"michezo\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"afya\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"burudani\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"kimataifa\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=data[data.category==\"uchumi\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    clean_txt = []\n",
    "    for i in tqdm(text):\n",
    "        txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in i]).split())\n",
    "        tokens = [word for word in txt.split()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        tokens = [word for word in tokens if len(word)>=3]\n",
    "        pre_proc_text = \" \".join([token for token in tokens])\n",
    "        clean_txt.append(pre_proc_text)\n",
    "    \n",
    "    return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23268/23268 [00:14<00:00, 1620.78it/s]\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing(data[\"content\"])\n",
    "y = data[\"category_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y to categorical\n",
    "y = to_categorical(y, num_classes=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame({\"content\":X, \"category\":data.category})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bodi utalii tanzania ttb imesema itafanya misa...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pendo fundisha mbeya rais john magufuri ametan...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mwandishi wetu singida benki nmb imetoa msaada...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timu taifa tanzania serengeti boys jana ilijiw...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agatha charles dar salaam aliyekuwa katibu bun...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content category\n",
       "0  bodi utalii tanzania ttb imesema itafanya misa...   uchumi\n",
       "1  pendo fundisha mbeya rais john magufuri ametan...  kitaifa\n",
       "2  mwandishi wetu singida benki nmb imetoa msaada...   uchumi\n",
       "3  timu taifa tanzania serengeti boys jana ilijiw...  michezo\n",
       "4  agatha charles dar salaam aliyekuwa katibu bun...  kitaifa"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset=temp[temp.category==\"kitaifa\"]\n",
    "# text=subset.content.values\n",
    "# words =\" \".join(text)\n",
    "# create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18614,), (18614,), (4654, 6), (4654, 6))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train.shape, y_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), max_features=10000, strip_accents='unicode', norm='l2')\n",
    "# Fit and transform trainining data\n",
    "X_train_features = np.array(vectorizer.fit_transform(X_train).todense())\n",
    "# Transform testing data\n",
    "X_test_features = np.array(vectorizer.transform(X_test).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"vectorizer.pickle\", \"wb\")\n",
    "pickle.dump(vectorizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018) # for reproducibility\n",
    "nb_classes = 6\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape = (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 256)               2560256   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,758,278\n",
      "Trainable params: 2,758,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "291/291 [==============================] - 11s 39ms/step - loss: 0.0346 - accuracy: 0.9857\n",
      "Epoch 2/20\n",
      "291/291 [==============================] - 11s 39ms/step - loss: 0.0308 - accuracy: 0.9868\n",
      "Epoch 3/20\n",
      "291/291 [==============================] - 10s 36ms/step - loss: 0.0295 - accuracy: 0.9868\n",
      "Epoch 4/20\n",
      "291/291 [==============================] - 11s 37ms/step - loss: 0.0306 - accuracy: 0.9871\n",
      "Epoch 5/20\n",
      "291/291 [==============================] - 11s 39ms/step - loss: 0.0300 - accuracy: 0.9872\n",
      "Epoch 6/20\n",
      "291/291 [==============================] - 15s 53ms/step - loss: 0.0274 - accuracy: 0.9871\n",
      "Epoch 7/20\n",
      "291/291 [==============================] - 11s 36ms/step - loss: 0.0338 - accuracy: 0.9860\n",
      "Epoch 8/20\n",
      "291/291 [==============================] - 15s 51ms/step - loss: 0.0292 - accuracy: 0.9860\n",
      "Epoch 9/20\n",
      "291/291 [==============================] - 13s 45ms/step - loss: 0.0291 - accuracy: 0.9869\n",
      "Epoch 10/20\n",
      "291/291 [==============================] - 15s 51ms/step - loss: 0.0287 - accuracy: 0.9870\n",
      "Epoch 11/20\n",
      "291/291 [==============================] - 17s 59ms/step - loss: 0.0267 - accuracy: 0.9873\n",
      "Epoch 12/20\n",
      "291/291 [==============================] - 14s 48ms/step - loss: 0.0260 - accuracy: 0.9871\n",
      "Epoch 13/20\n",
      "291/291 [==============================] - 14s 49ms/step - loss: 0.0260 - accuracy: 0.9873\n",
      "Epoch 14/20\n",
      "291/291 [==============================] - 15s 51ms/step - loss: 0.0249 - accuracy: 0.9875\n",
      "Epoch 15/20\n",
      "291/291 [==============================] - 17s 59ms/step - loss: 0.0259 - accuracy: 0.9873\n",
      "Epoch 16/20\n",
      "291/291 [==============================] - 14s 49ms/step - loss: 0.0252 - accuracy: 0.9883\n",
      "Epoch 17/20\n",
      "291/291 [==============================] - 14s 49ms/step - loss: 0.0243 - accuracy: 0.9874\n",
      "Epoch 18/20\n",
      "291/291 [==============================] - 16s 54ms/step - loss: 0.0261 - accuracy: 0.9872\n",
      "Epoch 19/20\n",
      "291/291 [==============================] - 16s 56ms/step - loss: 0.0237 - accuracy: 0.9874\n",
      "Epoch 20/20\n",
      "291/291 [==============================] - 14s 48ms/step - loss: 0.0269 - accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e672598b0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_features, y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18614, 10000)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.argmax(test_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       387\n",
      "           1       0.88      0.88      0.88      2048\n",
      "           2       0.94      0.95      0.95      1221\n",
      "           3       0.84      0.82      0.83       385\n",
      "           4       0.87      0.85      0.86       452\n",
      "           5       0.43      0.45      0.44       161\n",
      "\n",
      "    accuracy                           0.86      4654\n",
      "   macro avg       0.79      0.79      0.79      4654\n",
      "weighted avg       0.87      0.86      0.87      4654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_pred, np.argmax(y_test, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save(\"swahili.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3b519bf3-ca15-4bc4-ab08-740f07e9f322/assets\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-169f8f4c0d8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# pickle_out = open(\"model.pickle\",\"wb\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m__reduce__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m       return (pickle_utils.deserialize_model_from_bytecode,\n\u001b[1;32m--> 315\u001b[1;33m               pickle_utils.serialize_model_as_bytecode(self))\n\u001b[0m\u001b[0;32m    316\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m       \u001b[1;31m# SavedModel (and hence serialize_model_as_bytecode) only support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\saving\\pickle_utils.py\u001b[0m in \u001b[0;36mserialize_model_as_bytecode\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m           \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTarInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m           \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m           \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36msize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;34m\"\"\"Returns the size of the file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mstat\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m   \"\"\"\n\u001b[1;32m--> 910\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mstat_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mstat_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m   \"\"\"\n\u001b[1;32m--> 926\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_file_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"model.pickle\",\"wb\")\n",
    "pickle.dump(model,open(\"model.pickle\",\"wb\"))\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = load_model(\"swahili.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               2560256   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,758,278\n",
      "Trainable params: 2,758,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hypothesis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \" ni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.â€œTunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,â€ amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.\"\n",
    "test=' Akizungumza mara baada ya kutiwa saini kwa makubaliano hayo jana jioni Waziri wa Nchi Ofisi ya Waziri Mkuu Tawala za Mikoa na Serikali za Mitaa, Hawa Ghasia, alisema uwekezaji huo unatarajiwa kuleta manufaa kwa shirika hilo kwani uwekezaji wake pia utasaidia kutoa elimu kwa vitendo juu ya ufugaji kwa kada mbalimbali.Alisema miaka ya nyuma shirika hilo lilikuwa likisifika kwa ufugaji wa kuku kabla ya kusitisha shughuli hizo kutokana na uwepo wa sababu mbalimbali na kusababisha shirika hilo kukosa mapato yaliyokuwa yakitokana na uuzaji wa kuku na mayai.Mkurugenzi wa Shirika la Elimu Kibaha, Cyprian Mpemba alisema uwekezaji huo wa miaka 66, mbali na kuingizia shirika hilo kiasi cha Sh milioni 100 kila mwaka, pia utawawezesha kuvuna kiasi kingine kama hicho kama ada ya uingiaji katika eneo hilo.Alisema itachukua muda wa miaka nane hadi uwekezaji rasmi ukamilike katika eneo hilo ukihusisha idara mbalimbali ikiwemo ujenzi wa viwanda vya usindikaji wa nyama na utengenezaji wa soseji, huku kila baada ya miaka miwili wakikamilisha ujenzi wa eneo yakiwemo maeneo ya kufugia kuku hao.Mpemba alisema matarajio baada ya mika miwili ijayo mradi huo utaweza kuzalisha kuku milioni 2.7 huku wakitoa ajira kwa watu 120 na ukikamilika utazalisha kuku milioni 165 pamoja na kutoa ajira kwa watu 700 hatua aliyosema itasaidia kuleta mapinduzi katika ufugaji wa kuku hapa nchini sambamba na kutoa elimu kwa wanafunzi kutoka vyuo mbalimbali.Naye Mtendaji Mkuu wa kampuni hiyo Hesham Ewies alisema malengo yao ni kuibadilisha Kibaha kwa muda mfupi kwa kulifanya kuwa eneo linalosifika kutokana na shughuli za ufugaji wa kuku huku zaidi wakilenga kuondoa upungufu wa mahitaji ya bidhaa ya kuku yaliyopo kwa sasa nchini.Alisema watarajio yao baada ya miaka michache ijayo, Tanzania itaweza kusifika kwa usafirishaji wa bidhaa zitokanazo na kuku kwenda nje ya nchi na hivyo kuliongeza Taifa mapato.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(txt):\n",
    "    #print(txt)\n",
    "    text = preprocessing(txt)\n",
    "    text = \" \".join(text)\n",
    "    #print(text)\n",
    "    features = np.array(vectorizer.transform([text]).todense())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1889/1889 [00:00<00:00, 189075.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tst = test_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2class = dict([(value, key) for key, value in class_mapping.items()])\n",
    "np.argmax(hypothesis.predict(tst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(hypothesis.predict(tst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b4b8036d8587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dct' is not defined"
     ]
    }
   ],
   "source": [
    "dct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
